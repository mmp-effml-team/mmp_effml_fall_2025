{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2483c2e",
   "metadata": {},
   "source": [
    "## `Материалы кафедры ММП факультета ВМК МГУ. Введение в эффективные системы глубокого обучения.`\n",
    "\n",
    "## `Задание 04. Работа с большими моделями`\n",
    "\n",
    "#### Фамилия, имя:\n",
    "\n",
    "Дата выдачи: <span style=\"color:red\">__28.10.2025 21:30__</span>.\n",
    "\n",
    "Мягкий дедлайн: <span style=\"color:red\">__11.11.2025 11:11__</span>.\n",
    "\n",
    "Стоимость: __10 баллов__ (основная часть заданий) + __3 балла__ (дополнительные задания).\n",
    "\n",
    "<span style=\"color:red\">__В ноутбуке все клетки должны выполняться без ошибок при последовательном их выполнении.__</span>\n",
    "\n",
    "#### `Москва, 2025`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd88c12",
   "metadata": {},
   "source": [
    "Авторы задания: `@sir_rois` и `@jserdyuk`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8186c153-ce0a-4a03-8042-251cda8df0de",
   "metadata": {},
   "source": [
    "## `Постановка задачи`\n",
    "\n",
    "В данном задании вы научитесь обучать модель при помощи LoRA, а также реализуете CPU offloading.\n",
    "\n",
    "Используемая в задании модель Qwen/Qwen2.5-0.5B\n",
    "\n",
    "Используемый датасет - GSM8K (набор несложных математических задач с решением и ответом)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2facd3-901f-412c-b295-55d0a16a0053",
   "metadata": {},
   "source": [
    "## `Необходимая теория`\n",
    "\n",
    "### `LoRA (Low-Rank Adaptation)`\n",
    "**LoRA** — это метод адаптации больших языковых моделей, который позволяет обучать только небольшие низкоранговые матрицы вместо всех весов модели.  \n",
    "- Позволяет **экономить память и время обучения**.  \n",
    "- Подходит для **тонкой настройки (fine-tuning)** моделей без переобучения всех параметров.  \n",
    "- Обучение ограничивается несколькими адаптационными матрицами, а основная модель остаётся неизменной.\n",
    "\n",
    "Идея простая: вместо того чтобы обновлять все веса слоя `W`, мы добавляем **низкоранговую поправку**:\n",
    "\n",
    "$$\n",
    "W' = W + \\Delta W, \\quad \\Delta W = B A\n",
    "$$\n",
    "\n",
    "где:  \n",
    "- $A \\in \\mathbb{R}^{d_\\text{in} \\times r}$ и$ B \\in \\mathbb{R}^{r \\times d_\\text{out}}$ — **малые матрицы для адаптации**,  \n",
    "- $r \\ll d_\\text{in}, d_\\text{out}$ — низкий ранг,  \n",
    "- дополнительно можно использовать **масштабирование** и **dropout** для стабилизации обучения.\n",
    "\n",
    "\n",
    "\n",
    "### `CPU Offloading`\n",
    "**CPU Offloading** — это техника, когда часть модели (например, слои или параметры) хранится и обрабатывается на **CPU**, а не на GPU.  \n",
    "- Позволяет работать с **очень большими моделями**, которые не помещаются целиком на GPU.  \n",
    "- GPU используется только для активного вычисления, что снижает требования к памяти GPU.\n",
    "\n",
    "### `CPU Offloading with Overlap`\n",
    "\n",
    "**CPU Offloading with Overlap** — это усовершенствованная техника CPU Offloading, при которой **перемещение слоев между CPU и GPU происходит параллельно с вычислениями на GPU**.  \n",
    "\n",
    "- **Обычный CPU Offloading:** слой загружается на GPU → выполняется вычисление → выгружается обратно на CPU.  \n",
    "  Это может создавать паузы, потому что GPU ждёт загрузки слоя.  \n",
    "\n",
    "- **С Overlap:**  \n",
    "  - Пока GPU вычисляет текущий слой, **следующий слой уже подгружается с CPU**.  \n",
    "  - Это **сокращает простой GPU**, повышая скорость обработки.  \n",
    "\n",
    "Идея в том, чтобы **сглаживать задержки при работе с большими моделями**, которые не помещаются целиком на GPU.\n",
    "\n",
    "<span style=\"color:red\">ВНИМАНИЕ!</span>.\n",
    " Ячейки, в которых ожидается ваш письменный ответ, помечены (❓). Перед отправкой ноутбука проверьте, что вы ответили на все вопросы "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab6d9c-c361-4f21-8329-7a31de88cd03",
   "metadata": {},
   "source": [
    "## `Часть 1. Обучение LoRA (2 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620444f-613b-44d5-8fe2-bedc2fa2c482",
   "metadata": {},
   "source": [
    "Начнем с импорта необходимых библиотек, загрузки модели, разбиения датасета на трейн, валидацию и тест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa39fbbd-5773-4793-8b28-f205637a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "train_eval_test = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "eval_test_dataset = train_eval_test['test']\n",
    "train_dataset = train_eval_test['train']\n",
    "\n",
    "eval_test_split = eval_test_dataset.train_test_split(test_size=0.5, seed=42)\n",
    "test_dataset = eval_test_split['train']\n",
    "eval_dataset = eval_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212988f8-adad-4e19-b356-3cfb8af476f9",
   "metadata": {},
   "source": [
    "**Задание 1 (2 балла)**. Реализация LoRA. В этом задании необходимо дообучить LoRA и повысить качество после решения задач. Для выполнения данного задания нужно реализовать следующие пункты: \n",
    "1. Посмотрите на данные, которые хранятся в датасете и токенизируйте. Не забудьте, что для обучения модели лучше не использовать токены, соответствующие условию.\n",
    "1. Реализуйте функцию evaluate_generation, которая замеряет качество модели на тестовом датасете. В качестве метрики качества используйте `accuracy` предсказаний. Используйте few-shot с 3мя примерами решений для того, чтобы модель поняла формат, в котором вы ожидаете ответ.\n",
    "1. Реализуйте класс `LoRALinear`, который оборачивает модули модели, добавляя обучаемые матрицы для получения предсказаний. Заморозьте модель, оберните модули внимания (`q_proj`, `k_proj`, `v_proj`, `o_proj`)\n",
    "1. Замерьте качество модели до обучения, обучите модель, замерьте качество после обучения.\n",
    "1. Сделайте выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a16743-de53-4953-a877-f59f9932934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a single dataset example into the desired format.\n",
    "\n",
    "    Args:\n",
    "        sample (dict): One example containing fields like 'question' and 'answer'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - 'input_ids' (list): Token IDs of the input text.\n",
    "            - 'attention_mask' (list): Attention mask (1 for tokens to attend to, 0 to ignore).\n",
    "            - 'labels' (list): Token IDs used for loss computation (can contain -100 for tokens to ignore).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return {'input_ids': [], 'attention_mask': [], 'labels': []}\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize)\n",
    "eval_dataset = eval_dataset.map(tokenize)\n",
    "test_dataset = test_dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29785cd7-ee51-40e8-95b9-0784fbc4c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizerBase\n",
    "from datasets import Dataset\n",
    "\n",
    "def evaluate_generation(\n",
    "    model: PreTrainedModel,\n",
    "    dataset: Dataset,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    batch_size: int = 4,\n",
    "    few_shot_size: int = 3\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model's generation accuracy on a dataset.\n",
    "\n",
    "    Args:\n",
    "        model (PreTrainedModel): The language model to evaluate.\n",
    "        dataset (Dataset): The dataset containing examples to generate predictions for.\n",
    "        tokenizer (PreTrainedTokenizerBase): Tokenizer corresponding to the model.\n",
    "        batch_size (int, optional): Number of examples per batch. Defaults to 4.\n",
    "        few_shot_size (int, optional): Number of few-shot examples to prepend. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model's predictions on the dataset.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9af08e-518b-473c-b35f-24c01aa1bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, module, r=8, alpha=16, dropout=0.05):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE\n",
    "        self.module = module\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        return self.module(x)\n",
    "\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    pass\n",
    "    # Замените все слои, описанные в target_modules на LoRALinear\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "assert total_params == 495114112\n",
    "assert trainable_params == 1081344\n",
    "\n",
    "print(f\"Percentage of trainable params: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167a788-6a9a-4a6d-96da-8c6d2ba71bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Дегенеративная оценка до обучения:\")\n",
    "evaluate_generation(model, test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7af854-be98-4765-b365-0b9c1a46cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    ### YOUR CODE HERE\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    ### YOUR CODE HERE\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae334db-b173-4263-ae70-1433e8429d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Дегенеративная оценка после обучения:\")\n",
    "evaluate_generation(model, test_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138adf7c-8c07-4307-844c-356ba7d150a3",
   "metadata": {},
   "source": [
    "**Выводы** (❓):\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472eb364-ecf4-4766-ba70-742274bc3cb6",
   "metadata": {},
   "source": [
    "## `Часть 2. Реализация инференса с CPU Offloading (1 балл)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e214f8-f92b-4736-aca6-140fa2296aa2",
   "metadata": {},
   "source": [
    "Вновь начнем с загрузки модели и подготовки референсного предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78a5ca7-1899-4048-bab4-d3d6be2b7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "input_ids = tokenizer.encode(\"Hello, world!\", return_tensors=\"pt\")\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = Qwen2ForCausalLM(config)\n",
    "\n",
    "res_baseline = model(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4819a812-6c69-445f-9965-a480e7040b4e",
   "metadata": {},
   "source": [
    "Воспользуемся функией проверки того, что все слои на cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e06909-2a59-4fed-b377-62c278260d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def test_model_on_cpu(model: nn.Module) -> bool:\n",
    "    \"\"\"\n",
    "    Check that all layers in `model.model.layers` are on CPU.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to check, expected to have `model.model.layers`.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all layers are on CPU, False otherwise.\n",
    "    \"\"\"\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        for name, param in layer.named_parameters():\n",
    "            if param.device.type != 'cpu':\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "test_model_on_cpu(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87decff-c111-45c7-a5f8-84e45dfcc624",
   "metadata": {},
   "source": [
    "**Задание 2 (1 балл)**. Реализация `CPU Offloading`. Получить инференс модели про помощи простого CPU offloading, в ходе которого необходимый слой грузится с CPU на GPU, получается его предсказание и выгружается обратно. Для выполнения данного задания нужно реализовать следующие пункты: \n",
    "1. Реализуйте класс `OffloadBlock`, который оборачивает модули модели и модифицирует их `forward`. Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на GPU.\n",
    "1. Получите инференс, сравните с референсными значениями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ce7ccc-d0b8-4616-b7cf-49fde42aba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OffloadBlock(nn.Module):\n",
    "    def __init__(self, module: nn.Module, device:str = 'cuda'):\n",
    "        ### YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        ### YOUR CODE HERE\n",
    "        # Загружаем на GPU\n",
    "        # Вызываем forward модуля\n",
    "        out = ...\n",
    "        # Выгружаем на CPU\n",
    "        assert test_model_on_cpu(model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на gpu.\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res_offloading = model(input_ids=input_ids.to('cuda'))\n",
    "\n",
    "assert torch.allclose(res_baseline.logits, res_offloading.logits.cpu(), rtol=0, atol=1e-5), \"The outputs of the model before and after offloading do not match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b93339-7d3b-4606-9551-9d962d690a9e",
   "metadata": {},
   "source": [
    "## `Часть 3. Реализация обучения с CPU Offloading + checkpointing (1 балл)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e8861-75e6-42f1-ba12-f0f93ed46614",
   "metadata": {},
   "source": [
    "Вновь начнем с загрузки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4aab84-5e41-48f8-bb1e-0569f85727cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "input_ids = tokenizer.encode(\"Hello, world!\", return_tensors=\"pt\")\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = Qwen2ForCausalLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db84dca-7f64-43b0-8357-3d6491a15df9",
   "metadata": {},
   "source": [
    "Подготовим функцию, которая берет две модели и сравнивает их градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7001c2-1ed1-4fcf-bb91-0d45beb06171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если на этом этапе выпадает OOM - можете для дебага уменьшить число слоёв модели.\n",
    "# config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "# config.num_hidden_layers = 5\n",
    "# model = Qwen2ForCausalLM(config)\n",
    "\n",
    "def compare_grads(model1, model2):\n",
    "    for (n1, p1), (n2, p2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "        assert not (p1.grad is None or p2.grad is None)\n",
    "        assert torch.allclose(p1.grad.cpu(), p2.grad.cpu(), atol=1e-4)\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ead4d-7400-4c40-9954-a6f79459238d",
   "metadata": {},
   "source": [
    "**Задание 3 (1 балл)**. Реализация `CPU Offloading` + `Checkpointing`. Обучить модель при помощи CPU offloading и checkpointing, в ходе которого модели модели будут двигаться на gpu и обратно, а промежуточные результаты вычисляться заново при помощи ванильной реализации `checkpointing` из PyTorch. Для выполнения данного задания нужно реализовать следующие пункты: \n",
    "1. Реализуйте класс `OffloadBlockWithCheckpointing`, который оборачивает модули модели и модифицирует их `forward`. В ходе `forward` нужно получить предсказания модуля с помощью `checkpoint`. Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на GPU.\n",
    "1. Возьмите также модель без оффлоадинга.\n",
    "1. Совершите по три шага обучения для обеих моделей на идентичных входных сэмплах.\n",
    "1. При помощи функции `compare_grads` проверьте, что градиенты после трех шагов совпадают.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12f2a0ef-ccba-49eb-9515-c075628b61bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class OffloadBlockWithCheckpointing(nn.Module):\n",
    "    def __init__(self, module: nn.Module, layer=-1, device='cuda'):\n",
    "        ### YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        ### YOUR CODE HERE\n",
    "        assert test_model_on_cpu(model)\n",
    "        return out\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Совершите по три шага обучения для обеих моделей на идентичных входных сэмплах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a34ddfde-d069-4814-a596-85efec4c2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_grads(model_wo_offloading, model_w_offloading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25738de-d6d9-4b8f-a688-b8ee5321c610",
   "metadata": {},
   "source": [
    "## `Часть 4. Реализация обучения при помощи torch.autograd.Function (2 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea98d1-aa27-447e-961a-13fd968d13e1",
   "metadata": {},
   "source": [
    "Вновь начнем с загрузки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3f9ff4-ffd7-4c56-9ad8-83444be08713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "input_ids = tokenizer.encode(\"Hello, world!\", return_tensors=\"pt\")\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = Qwen2ForCausalLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad009ca-cf7c-45ed-84e6-6d31a78f9369",
   "metadata": {},
   "source": [
    "Подготовим функцию, которая берет две модели и сравнивает их градиенты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee6e0dc-cfc2-421e-8da9-b5233117ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если на этом этапе выпадает OOM - можете для дебага уменьшить число слоёв модели.\n",
    "# config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "# config.num_hidden_layers = 5\n",
    "# model = Qwen2ForCausalLM(config)\n",
    "\n",
    "def compare_grads(model1, model2):\n",
    "    for (n1, p1), (n2, p2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "        assert not (p1.grad is None or p2.grad is None)\n",
    "        assert torch.allclose(p1.grad.cpu(), p2.grad.cpu(), atol=1e-4)\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519be42-1275-4cc3-95a3-217f88c22eac",
   "metadata": {},
   "source": [
    "**Задание 4 (2 балла)**. Реализация обучения при помощи `torch.autograd.Function`. Обучить модель, реализовав методы `offloading` и `checkpointing` внутри методов функции. Для выполнения данного задания нужно реализовать следующие пункты: \n",
    "1. Реализуйте класс `OffloadFunction`, внутри которого содержатся реализации функций `forward` и `backward`. Внутри этих функция должен быть реализован `offloading` и `checkpointing`.\n",
    "1. Реализуйте класс `OffloadBlockWithFunction`, который оборачивает модули модели и модифицирует их `forward`, применяя функцию `OffloadFunction`. В ходе forward нужно получить предсказания модуля с помощью `checkpoint`. В данном задани пользоваться реализацией `checkpoint` из PyTorch <span style=\"color:red\">запрещено</span>. Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на GPU.\n",
    "1. Возьмите также модель без оффлоадинга.\n",
    "1. Совершите по три шага обучения для обеих моделей на идентичных входных сэмплах.\n",
    "1. При помощи функции compare_grads проверьте, что градиенты после трех шагов совпадают.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078693a-b735-4e78-9ea5-042f9f67fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward( ... ):\n",
    "        ### YOUR CODE HERE\n",
    "        return outputs if isinstance(outputs, tuple) else (outputs,)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward( ... ):\n",
    "        ### YOUR CODE HERE\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111daf6-0aee-4646-88ca-22edf58a19d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadBlockWithFunction(nn.Module):\n",
    "    def __init__(self, module: nn.Module, device='cuda'):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        ### YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Совершите по три шага обучения для обеих моделей на идентичных входных сэмплах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d5aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_grads(model_wo_offloading, model_w_offloading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d27a5c-ccdd-4962-97ed-b6a16b5ad46a",
   "metadata": {},
   "source": [
    "## `Часть 5. Реализация обучения с оффлоадингом и оверлапом (4 балла)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b70c5c-e392-4fa3-9f22-9dd891c13a32",
   "metadata": {},
   "source": [
    "Ну вы поняли"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44290a2c-1bf7-4162-8e38-72bb0831c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "input_ids = tokenizer.encode(\"Hello, world!\", return_tensors=\"pt\")\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "model = Qwen2ForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f1464-2c67-4baf-89a0-c836237bdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если на этом этапе выпадает OOM - можете для дебага уменьшить число слоёв модели.\n",
    "# config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "# config.num_hidden_layers = 5\n",
    "# model = Qwen2ForCausalLM(config)\n",
    "\n",
    "def compare_grads(model1, model2):\n",
    "    for (n1, p1), (n2, p2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "        assert not (p1.grad is None or p2.grad is None)\n",
    "        assert torch.allclose(p1.grad.cpu(), p2.grad.cpu(), atol=1e-4)\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b54624-9dd6-4e82-94d9-37bef2a630e6",
   "metadata": {},
   "source": [
    "**Задание 5 (3.5 балла)** Реализуйте механизм асинхронного оффлоадинга для модели Qwen2.5. Во время вычислений текущего слоя модель должна заранее подгружать следующий слой и асинхронно выгружать предыдущий, используя несколько CUDA Stream и CUDA Events для синхронизации. Для повышения эффективности необходимо реализовать буфер из трёх слоёв и использовать pinned memory при передаче данных между CPU и GPU.\n",
    "\n",
    "\n",
    "1. Реализуйте класс `OverlapOffloadFunction`, наследующий `torch.autograd.Function`.\n",
    "   В нём должны быть реализованы методы `forward` и `backward`, обеспечивающие:\n",
    "   1. Асинхронную загрузку следующего слоя (prefetch) во время вычислений текущего;\n",
    "   1. Асинхронную выгрузку предыдущего слоя (offload);\n",
    "   1. Синхронизацию потоков при помощи `torch.cuda.Event` и `wait_event`;\n",
    "   1. Использование pinned memory для всех данных, передаваемых между CPU и GPU;\n",
    "   1. Корректное сохранение необходимых тензоров для `backward`.\n",
    "<br><br>\n",
    "1. Класс `OverlapOffloadBlockWithFunction`, оборачивающий блоки трансформера.\n",
    "   Он должен:\n",
    "   1. Инициализировать два CUDA Streams — `prefetch_stream` и `offload_stream`;\n",
    "   1. При каждом шаге:\n",
    "      * Выполнять вычисления текущего слоя;\n",
    "      * Загружать следующий слой на GPU в `prefetch_stream`;\n",
    "      * Выгружать предыдущий слой обратно на CPU в `offload_stream`;\n",
    "   1. Корректно управлять жизненным циклом потоков и событий (`event.record`, `wait_event`);\n",
    "   1. Удерживать слои embedding, norm и lm_head постоянно на GPU\n",
    "<br><br>\n",
    "1. Возьмите также модель без оффлоадинга.\n",
    "1. Совершите по три шага обучения для обеих моделей на идентичных входных сэмплах.\n",
    "1. При помощи функции compare_grads проверьте, что градиенты после трех шагов совпадают.\n",
    "\n",
    "\n",
    "**Практические рекомендации**\n",
    "1. Можете воспользоваться `torch.profiler` или `nsys` для проверки, что операции `memcpyHtoD` и `kernel` выполняются параллельно.\n",
    "1. Убедитесь, что pinned memory действительно используется: без неё non_blocking=True не даст эффекта.\n",
    "1. Не выгружайте слой, пока его тензоры ещё участвуют в вычислениях.\n",
    "1. Для стабильности можно добавить мягкую синхронизацию: `wait_stream(prefetch_stream)` перед завершением итерации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a6b7c-e2f4-4f03-a6d3-7aa183ff0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadFunctionWithOverlap(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward( ... ):\n",
    "        ### YOUR CODE HERE\n",
    "        return outputs if isinstance(outputs, tuple) else (outputs,)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward( ... ):\n",
    "        ### YOUR CODE HERE\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9b667-060b-4188-89b9-d50c0fef73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OffloadBlockWithOverlapFunction(nn.Module):\n",
    "    def __init__(self, module: nn.Module, device='cuda'):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        ### YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a518ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оберните слои трансформера. Слои с нормализацией, эмбедингами и lm_head всегда держите на gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e13d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Совершите по три шага обучения для обеих моделей на идентичных входных сэмплах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63937134",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_grads(model_wo_offloading, model_w_offloading)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803102e6",
   "metadata": {},
   "source": [
    "**Задание 6 (0.5 балла)**\n",
    "\n",
    "Приложите скриншот результатов работы профилировщика, на котором виден overlap загрузки и вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559e336",
   "metadata": {},
   "source": [
    "## `Часть 6. Бонусы (3 балла)`\n",
    "\n",
    "1. (1 балл). Покажите, что написанные вами функции и классы действительно помогают работать с большими моделями. Для этого покажите, что вы умеете работать с `Qwen2.5-7B` и потребляете меньше 10 ГБ видеопамяти, при этом GPU работает.\n",
    "2. (1 балл). Составьте сводную таблицу, сколько времени занимает forward и backward, а также сколько памяти требует каждый из реализованных вами методов. В качестве бейзлайнов возьмите полностью GPU (если возможно) и полностью CPU реализации.\n",
    "3. (1 балл). Реализуйте offloading состояния оптимизатора. Проверьте, что использование такой версии оптимизатора приводит к тому, что обучение совпадает с обычной версией."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
