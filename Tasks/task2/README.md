# `Введение в Эффективные Системы Машинного обучения`

## `Задание 02. Ускорение обучения`


Дата выдачи: __19 сентября__

Мягкий дедлайн: __03 октября 02:00__

Стоимость: __10 баллов__ (основная часть заданий) + __1 балл__ (дополнительные задания).

#### `Москва, 2025`

##### По всем вопросам писать `@trandelik`

Задание основано на [аналогичном задании курса ШАДа](https://github.com/mryab/efficient-dl-systems/tree/main/week03_fast_pipelines/homework).

### `Формат сдачи`

Архив с кодом + отчет в формате pdf

### `Setup`

Во всех частях задания ваша задача обучать GPT-2-like модель на [WikiText-103](https://disk.360.yandex.ru/d/fL9HkaunY7o29g).
Необходимые для выполнения библиотеки с версиями можно найти в файле `requirements.txt` (но вы можете использовать любое другое окружение, **если укажете его**). **ОБЯЗАТЕЛЬНО УКАЖИТЕ МОДЕЛЬ GPU**, которую используете.

Часто когда нужно ставить эксперименты и проверять гипотезы удобно все вынести в аргументы командной строки с помощью библиотеки `argparse`. Для вашего удобства в файле `train.py` мы подготовили основные аргументы, которые вам могут потребоваться. Чувствуйте свободу в изменениях, но учитывайте что мы заложили неэффективности в код, которые вам предстоит найти. Поэтому не забудьте что вы поменяли для соотвествующего задания.

### `Language Modeling (0 баллов)`

Реализуйте `LMAccuracy` и `LMCrossEntropyLoss` из файла `utils.py` (можно взять из задания по RNN курса ВвГО)

Также реализуйте `PositionalEncoding` из файла `positional_encoding.py` (можно адаптировать из задания по Conformer курса ВвГО)

### `Automatic Mixed Precision from scratch (4 балла)`

Шаблоны всех классов находятся в `amp.py`. Вы можете редактировать любые файлы задания или создавать новые, если считаете нужным.

**Задание**:
 - Напишите рукописную реализацию `StaticGradScaler` (1 балл)
 - Напишите рукописную реализацию `DynamicGradScaler` (1 балл)
 - Напишите рукописную реализацию `Autocast` (2 балла)

**Пояснения**: Ваша задача реализовать [loss scaling](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling). Loss scaling используется для решения проблемы нехватки точности при вычислении градиентов в fp16: когда элемент градиента близок к нулю в fp32, то в fp16 он может оказаться равен нулю. Чтобы решить эту проблему предлагается использовать следующий подход:
- Делаем forward pass и считаем loss
- Умножаем loss на какое-то число (scale)
- Зовем `.backward()`
- Обновляем параметры модели, используя градиенты, которые были разделены на scale
    
В `StaticGradScaler` должно использоваться одно и то же значение scale для всего обучения. В то время как в `DynamicGradScaler` значение scale пересчитывается на каждой итерации в зависимости от наличия nan/inf в градиенте.

Также вам требуется реализовать контекстный менеджер `Autocast`, который переводит вычисления линейных слоев в заданный тип данных, а нормализации вычисляет всегда в fp32.

**Для получения полного балла за задание необходимо**:
- Произвести сравнение качества работы обучения в следующих сетапах: обучение в fp32, обучение в fp16, обучение с использованием AMP и `StaticLossScaler`, обучение с использование AMP и `DynamicLossScaler`
- Также необходимо сравнить время одного вычисления градиентов в указанных выше сетапах
- Для получения баллов за `StaticGradScaler` и `DynamicGradScaler` вы можете использовать `torch.amp.autocast`, но не можете использовать `torch.amp.GradScaler()`
- Для получения баллов за `Autocast` вы можете использовать `torch.amp.GradScaler()`, но не можете использовать `torch.amp.autocast`

**Замечания**:
- При использовании всех вариантов обучения (возможно, кроме fp16) модель не должна расходиться
- При использовании `DynamicGradScaler` вы должны увидеть прирост качества

### `Efficient Data Loading (4 балла)`

Шаблоны всех классов находятся в `data.py`. Вы можете редактировать любые файлы задания или создавать новые, если считаете это нужным.

**Задание**:
- Напишите `StandardDataset` и `collate_fn` (1 балл)
- Напишите `BalancedBatchSampler` (1.5 балла)
- Напишите `SequencedDataset` (1.5 балла)

**Пояснения**: 
- Вам уже дана наивная реализация `BaseDataset`, которую вы вероятно использовали в прошлом пункте. В ней все примеры из датасета выравниваются до одинаковой длины
- В классе `StandardDataset` вы должны оставить только укорачивание до заданной длины, при этом padding должен выполняться до максимальной длины последовательности в батче с помощью `collate_fn`
- Для следующего подхода вам необходимо реализовать `BalancedBatchSampler`. Вам нужно реализовать сэмплер, который будет объединять в один батч сэмплы похожей длины. При этом батчи должны быть случайными, в то же время необходимо убедиться, что разница между самой длинной и самой короткой последовательностью в батче должны быть не больше `k`. Часть батчей может быть короче, чем заданный размер батча, чтобы удовлетворять этому ограничению.  Для реализации `BalancedBatchSampler` удобно наслодоваться от базового класса [Sampler](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Sampler), который можно подавать в `DataLoader`. Метод `__init__` должен работать за линейное от размера датасета время. Сложность метода `__iter__` должна быть независимой от размера датасета и линейна по размеру батча. Чтобы достичь этого, при обработке датасета можно сложить данные в хештаблицу с ключами равными длине примеров, а значениями контейнерами содержащими индексы сэмплов в датасете заданной длины. 
- `SequencedDataset`реализует следующую логику: он собирает все сэмплы заданного батча в одну длинную последовательность и выдает дополнительную информацию, в которой хранится, где какой сэмпл начинается (например, attention маски). Если последний сэмпл батча оказывается слишком длинный, то его можно либо обрезать, либо выбросить. Как можно заметить, данный класс уже является не совсем датасетом, а больше похож на `DataLoader` и используется соответственно. Для удобства реализации мы советуем использовать `IterableDataset` в качестве базового класса, это удобно, когда неизвестно, сколько элементов в датасете будет по итогу.


**Для получения полного балла за задание необходимо**:
- Произвести сравнение времени работы **всего обучения** и потребляемой памяти каждого из датасетов.

### `Profiling (1 балл)`

Зафиксируйте оптимальный с вашей точки зрения сетап из прошлых пунктов, дальнейшее задание выполняйте в нем.

**Задание**
- В коде оставлено несколько неэффективностей. Используя профилировщик, вам нужно найти их и исправить
- Напишите, какие неэффективности вы нашли, объясните, почему это неэффективно и покажите, как изменился профиль обучения после их устранения
- За первые 5 найденных неэффективностей вы получите по 0.2 балла

### `Отчет (1 балл)`
Соберите все результаты в небольшой отчет. Насколько в сумме удалось ускорить обучение? Какое ускорение дал каждый из этапов?


### `Бонусы (1 балл)`
- (0.75 балла) В `torch.nn.TransformerDecoderLayer` используются `LayerNorm`. Замените их на `FusedLayerNorm` из [Apex](https://nvidia.github.io/apex/layernorm.html) и изучите изменения в скорости работы и потребляемой памяти от этой замены
- (0.25 балла) Используйте `torch.compile` и изучите изменения в скорости работы и потребляемой памяти при его использовании
