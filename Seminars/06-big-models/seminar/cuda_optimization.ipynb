{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50358e76",
   "metadata": {},
   "source": [
    "# `Материалы кафедры ММП факультета ВМК МГУ. Введение в Эффективные Системы Глубокого Обучение.`\n",
    "\n",
    "# `Семинар 06.3. Граф вычислений с точки зрения эффективности: выполнение CUDA операций`\n",
    "\n",
    "### `Материалы составил Феоктистов Дмитрий (@trandelik)`\n",
    "\n",
    "#### `Москва, Осенний семестр 2025`\n",
    "\n",
    "О чём можно узнать из этого ноутбука:\n",
    "\n",
    "* `Cuda Events` и их использование для замера времени\n",
    "* `Cuda Streams` для достижения overlap\n",
    "* `Cuda Graphs` для уменьшения overhead-а API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9255081",
   "metadata": {},
   "source": [
    "### `Setup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7bd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313af644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"CUDA is not available. This notebook requires a GPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45528732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2641ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"profiler_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    print(f\"Created directory: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b51caf",
   "metadata": {},
   "source": [
    "### `CUDA Events`\n",
    "\n",
    "`CUDA Events` — это маркеры в потоке выполнения CUDA, которые позволяют **точно измерять время выполнения операций на GPU**. Операции CUDA по своей природе **асинхронны**. Когда вы вызываете `model(data)`, Python немедленно возвращает управление вашему скрипту, в то время как фактические вычисления ставятся в очередь для выполнения на GPU.\n",
    "\n",
    "Использование `time.time()` для измерения производительности GPU некорректно, так как оно измеряет только время, которое CPU тратит на *запуск* операции, а не время, которое GPU тратит на её *выполнение*.\n",
    "\n",
    "Правильным инструментом является `torch.cuda.Event`. Алгоритм работы:\n",
    "1. Создать начальное и конечное события: `start = torch.cuda.Event(enable_timing=True)`.\n",
    "2. Записать начальное событие: `start.record()`.\n",
    "3. Выполнить операции на GPU.\n",
    "4. Записать конечное событие: `end.record()`.\n",
    "5. Синхронизировать CPU, чтобы дождаться завершения событий: `torch.cuda.synchronize()`.\n",
    "6. Получить время: `elapsed_ms = start.elapsed_time(end)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f014ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10240\n",
    "hidden_size = 1024\n",
    "output_size = 512\n",
    "batch_size = 1280\n",
    "num_batches = 1\n",
    "\n",
    "model = SimpleModel(input_size, hidden_size, output_size).to(device)\n",
    "cpu_data = [torch.randn(batch_size, input_size, pin_memory=True) for _ in range(num_batches)]\n",
    "\n",
    "dummy_input = torch.randn(batch_size, input_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b91544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up GPU...\n",
      "Incorrect time (time.time() without sync): 5.2507 ms\n",
      "Correct time (torch.cuda.Event): 2.4279 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"Warming up GPU...\")\n",
    "for _ in range(10):\n",
    "    _ = model(dummy_input)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# --- 1. The WRONG way (using time.time) ---\n",
    "# This measures CPU launch time, which is very small and misleading\n",
    "t0 = time.time()\n",
    "_ = model(dummy_input)\n",
    "t1 = time.time()\n",
    "print(f\"Incorrect time (time.time() without sync): {(t1 - t0) * 1000:.4f} ms\")\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# --- 2. The CORRECT way (using CUDA Events) ---\n",
    "start_event = torch.cuda.Event(enable_timing=True)\n",
    "end_event = torch.cuda.Event(enable_timing=True)\n",
    "start_event.record()\n",
    "_ = model(dummy_input)\n",
    "end_event.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "print(f\"Correct time (torch.cuda.Event): {elapsed_time_ms:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f91f64",
   "metadata": {},
   "source": [
    "### `Baseline`\n",
    "\n",
    "В оставшейся части этого ноутбука мы будем использовать `time.time()` для замера времени всего цикла инференса по батчам с вызовом `torch.cuda.synchronize()` в конце, чтобы сделать замер более честным. Так измеряется полное реальное время выполнения, что является корректным способом сравнения общей производительности различных стратегий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d251d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronous_execution(cpu_data, model, num_batches):\n",
    "    for data in cpu_data:\n",
    "        data_gpu = data.to(device)\n",
    "        output_gpu = model(data_gpu)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88b61d",
   "metadata": {},
   "source": [
    "### `CUDA Streams`\n",
    "\n",
    "`CUDA Stream` (поток CUDA) — это последовательность операций, выполняемых на GPU в определенном порядке. По умолчанию в PyTorch используется один глобальный поток (`default stream`). Используя несколько потоков, мы можем попросить GPU выполнять независимые последовательности операций **одновременно**. \n",
    "\n",
    "Основная цель — **совместить** передачу данных и вычисления, чтобы скрыть задержки и максимально загрузить GPU, который в противном случае простаивал бы в ожидании данных.\n",
    "\n",
    "Ключевые элементы:\n",
    "- **`torch.cuda.Stream()`**: Создает новый, не-дефолтный поток.\n",
    "- **`with torch.cuda.stream(s):`**: Контекстный менеджер, который направляет все CUDA-операции внутри блока в указанный поток `s`.\n",
    "- **`non_blocking=True`**: Важнейший аргумент для метода `.to()`, который делает копирование данных асинхронным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0ad7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamed_execution(cpu_data, model, num_batches):\n",
    "    compute_stream = torch.cuda.Stream()\n",
    "    data_stream = torch.cuda.Stream()\n",
    "        \n",
    "    for i in range(num_batches):\n",
    "        with torch.cuda.stream(data_stream):\n",
    "            data_gpu = cpu_data[i].to(device, non_blocking=True)\n",
    "        with torch.cuda.stream(compute_stream):\n",
    "            output_gpu = model(data_gpu)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9977bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streamed_execution_v2(cpu_data, model, num_batches):\n",
    "    compute_stream = torch.cuda.Stream()\n",
    "    data_stream = torch.cuda.Stream()\n",
    "    with torch.cuda.stream(data_stream):\n",
    "        data_gpu = cpu_data[0].to(device, non_blocking=True)\n",
    "        \n",
    "    for i in range(num_batches):\n",
    "        compute_stream.wait_stream(data_stream)\n",
    "        with torch.cuda.stream(compute_stream):\n",
    "            output_gpu = model(data_gpu)\n",
    "        if i < num_batches - 1:\n",
    "            with torch.cuda.stream(data_stream):\n",
    "                data_gpu = cpu_data[i + 1].to(device, non_blocking=True)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aebe3f",
   "metadata": {},
   "source": [
    "### `CUDA Graphs`\n",
    "\n",
    "Даже при использовании потоков CPU все равно должен отправлять на запуск каждое отдельное ядро (kernel) для каждого батча. Этот процесс запуска имеет небольшие, но ненулевые накладные расходы. Для моделей с множеством мелких операций или на очень быстрых GPU эти накладные расходы CPU могут стать узким местом.\n",
    "\n",
    "**CUDA Graphs** решают эту проблему, позволяя «захватить» последовательность операций GPU и затем «перезапускать» её одной командой. Драйвер GPU может оптимизировать этот предопределенный граф для чрезвычайно быстрого выполнения, почти полностью устраняя накладные расходы CPU на запуск ядер.\n",
    "\n",
    "**Ключевые ограничения:**\n",
    "- Операции внутри графа (архитектура модели) должны быть статичными.\n",
    "- **Размеры** входных и выходных тензоров не должны меняться между воспроизведениями.\n",
    "\n",
    "План работы:\n",
    "1.  **Warmup**: запустить код обычным способом пару рах.\n",
    "2.  **Capture**: создать статичные тензоры для входа/выхода. Записать запуск модели в `torch.cuda.CUDAGraph`.\n",
    "3.  **Replay**: В цикле копировать новые данные в статичный вход и звать `graph.replay()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0164d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph(batch_size, input_size, model):\n",
    "    # 1. Define static tensors. Their memory locations will be captured by the graph.\n",
    "    static_input = torch.randn(batch_size, input_size, device=device)\n",
    "    \n",
    "    # 2. Warmup: Run the model once to prepare the GPU and caches.\n",
    "    # This is important before capturing the graph.\n",
    "    torch.cuda.synchronize()\n",
    "    for _ in range(3):\n",
    "        _ = model(static_input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 3. Capture the graph\n",
    "    g = torch.cuda.CUDAGraph()\n",
    "    with torch.cuda.graph(g):\n",
    "        static_output = model(static_input)\n",
    "    torch.cuda.synchronize()\n",
    "    return g, static_input\n",
    "\n",
    "def run_graph(cpu_data, graph, static_input):\n",
    "    torch.cuda.synchronize()\n",
    "    for data in cpu_data:\n",
    "        # Copy new data into the static input tensor\n",
    "        static_input.copy_(data) \n",
    "        # Replay the captured graph. This is much faster than launching kernels individually.\n",
    "        graph.replay()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71fbb8",
   "metadata": {},
   "source": [
    "#### `В чем разница между CUDA Graphs и torch.compile?`\n",
    "\n",
    "Хотя обе технологии нацелены на ускорение, они работают на разных уровнях и решают немного разные задачи:\n",
    "\n",
    "| Характеристика | `torch.cuda.graph` (ручное использование) | `torch.compile()` |\n",
    "| :--- | :--- | :--- |\n",
    "| **Основная цель** | Устранение накладных расходов CPU на запуск ядер. | Комплексная JIT-компиляция модели: слияние операций (fusion), генерация эффективных ядер, минимизация Python overhead. |\n",
    "| **Уровень** | Низкоуровневый API CUDA. | Высокоуровневый API PyTorch. |\n",
    "| **Гибкость** | Очень низкая. Требует статической архитектуры и **статических размеров тензоров**. | Высокая. Может обрабатывать динамические размеры тензоров (с перекомпиляцией) и условную логику. |\n",
    "| **Простота** | Требует ручной настройки, разогрева, захвата и воспроизведения. | Просто обернуть модель: `compiled_model = torch.compile(model)`. |\n",
    "| **Когда использовать** | В inference-циклах, где задержка запуска CPU является доказанным узким местом. | Почти всегда. Это рекомендуемый способ ускорения моделей в PyTorch 2.x. `torch.compile` **может использовать CUDA Graphs внутри себя** как одну из стратегий оптимизации. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058abace",
   "metadata": {},
   "source": [
    "`torch.compile` — это универсальный и предпочтительный инструмент. Ручное использование `CUDA Graphs` — это более специализированная техника для достижения максимальной производительности в очень специфических, статичных сценариях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d09dc",
   "metadata": {},
   "source": [
    "### `Сравнение производительности`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d6e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance(batch_size, input_size=1024, hidden_size=1024, output_size=512, num_batches=1280):\n",
    "    print(f\"\\n--- Сравнение производительности для batch_size = {batch_size} ---\")\n",
    "\n",
    "    model = SimpleModel(input_size, hidden_size, output_size).to(device)\n",
    "    # pin_memory - важно для асинхронной передачи данных\n",
    "    cpu_data = [torch.randn(batch_size, input_size, pin_memory=True) for _ in range(num_batches)]\n",
    "\n",
    "    print(\"Подготовка и разогрев GPU...\")\n",
    "    for _ in range(10):\n",
    "        _ = model(torch.randn(batch_size, input_size, device=device))\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    print(\"Измерение синхронного выполнения (Baseline)...\")\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof_sync:\n",
    "        with record_function(f\"synchronous_run_{batch_size}\"):\n",
    "            start_time = time.time()\n",
    "            synchronous_execution(cpu_data=cpu_data, model=model, num_batches=num_batches)\n",
    "            sync_duration = time.time() - start_time\n",
    "    prof_sync.export_chrome_trace(os.path.join(log_dir, f\"sync_trace_{batch_size}.json\"))\n",
    "    \n",
    "    print(\"Измерение выполнения с CUDA Streams...\")\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof_async:\n",
    "        with record_function(f\"streamed_run_{batch_size}\"):\n",
    "            start_time = time.time()\n",
    "            streamed_execution(cpu_data=cpu_data, model=model, num_batches=num_batches)\n",
    "            stream_duration = time.time() - start_time\n",
    "    prof_async.export_chrome_trace(os.path.join(log_dir, f\"stream_trace_{batch_size}.json\"))\n",
    "\n",
    "\n",
    "    print(\"Измерение выполнения с CUDA Graphs...\")\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], profile_memory=True, record_shapes=True) as prof_graph:\n",
    "        with record_function(\"cuda_graph_run\"):\n",
    "            graph, static_input = prepare_graph(batch_size=batch_size, input_size=input_size, model=model)\n",
    "            start_time = time.time()\n",
    "            run_graph(cpu_data=cpu_data, graph=graph, static_input=static_input)\n",
    "            graph_duration = time.time() - start_time\n",
    "    \n",
    "    prof_graph.export_chrome_trace(os.path.join(log_dir, f\"graph_trace_{batch_size}.json\"))\n",
    "\n",
    "    print(\"\\n--- Итоги ---\")\n",
    "    print(f\"Синхронное (Baseline):  {sync_duration:.4f} секунд\")\n",
    "    print(f\"CUDA Streams:           {stream_duration:.4f} секунд (Ускорение: {sync_duration / stream_duration:.2f}x)\")\n",
    "    print(f\"CUDA Graphs:            {graph_duration:.4f} секунд (Ускорение: {sync_duration / graph_duration:.2f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2297eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сравнение производительности для batch_size = 1280 ---\n",
      "Подготовка и разогрев GPU...\n",
      "Измерение синхронного выполнения (Baseline)...\n",
      "Измерение выполнения с CUDA Streams...\n",
      "Измерение выполнения с CUDA Graphs...\n",
      "\n",
      "--- Итоги ---\n",
      "Синхронное (Baseline):  1.2326 секунд\n",
      "CUDA Streams:           0.8318 секунд (Ускорение: 1.48x)\n",
      "CUDA Graphs:            1.1350 секунд (Ускорение: 1.09x)\n"
     ]
    }
   ],
   "source": [
    "compare_performance(batch_size=1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38600f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сравнение производительности для batch_size = 16 ---\n",
      "Подготовка и разогрев GPU...\n",
      "Измерение синхронного выполнения (Baseline)...\n",
      "Измерение выполнения с CUDA Streams...\n",
      "Измерение выполнения с CUDA Graphs...\n",
      "\n",
      "--- Итоги ---\n",
      "Синхронное (Baseline):  0.5552 секунд\n",
      "CUDA Streams:           0.6231 секунд (Ускорение: 0.89x)\n",
      "CUDA Graphs:            0.1401 секунд (Ускорение: 3.96x)\n"
     ]
    }
   ],
   "source": [
    "compare_performance(batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6a35ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Сравнение производительности для batch_size = 1 ---\n",
      "Подготовка и разогрев GPU...\n",
      "Измерение синхронного выполнения (Baseline)...\n",
      "Измерение выполнения с CUDA Streams...\n",
      "Измерение выполнения с CUDA Graphs...\n",
      "\n",
      "--- Итоги ---\n",
      "Синхронное (Baseline):  0.4950 секунд\n",
      "CUDA Streams:           0.5571 секунд (Ускорение: 0.89x)\n",
      "CUDA Graphs:            0.1029 секунд (Ускорение: 4.81x)\n"
     ]
    }
   ],
   "source": [
    "compare_performance(batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
